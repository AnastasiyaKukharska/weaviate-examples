{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0lQLjQFU5Dx"
      },
      "source": [
        "# Basic QA Pipeline with WeaviateDocumentStore\n",
        "\n",
        "[Haystack](https://haystack.deepset.ai/overview/intro), a frramework for NLP applications, provides a range of [Document Store](https://haystack.deepset.ai/components/document-store) integrations. [Weaviate]() is one of the possible DocumentStore integrations. Weaviate is a vector database which enables fast and scalable vector storage, search and retrieval. In this tutorial we will be walking through how you could use the `WeaviateDocumentStore` within a Haystack Question Answering pipeline. Here, we will be using the Harry Potter wiki data in CSV format to answer some questions about the wizarding world ⚡️\n",
        "\n",
        "This demo is built by Tuana Çelik (Deepset) and Laura Ham (Weaviate)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fiv8Oe0K_Xl3"
      },
      "source": [
        "### First: enable the GPU runtime\n",
        "Make sure you enable the GPU runtime to experience decent speed in this tutorial.\n",
        "**Runtime -> Change Runtime type -> Hardware accelerator -> GPU**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/master/docs/_src/img/colab_gpu_runtime.jpg\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkLHL5BFWeLv",
        "outputId": "ad9a71cd-344b-49bc-f378-984f12aa9ea8"
      },
      "outputs": [],
      "source": [
        "# Make sure you have a GPU running\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMcMKDmGWt72"
      },
      "source": [
        "### Install Haystack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XmREH8aW0qd",
        "outputId": "208ace5a-637e-4c8c-894a-bbe71ffcda9f"
      },
      "outputs": [],
      "source": [
        "# Install the latest Haystack from GitHub's master\n",
        "!pip install --upgrade pip\n",
        "!pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab,weaviate]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibwCUDKHWhII"
      },
      "source": [
        "## 1. Write the data into a list of objects of Document format\n",
        "\n",
        "Haystack expects text to come in [Document](https://haystack.deepset.ai/reference/primitives) format. So, first we have to prepare the data to write to WeaviateDocumentStore. We read each line of the CSV and write them into a `dict`. Each object we write will have a \"content\" and \"meta\" field to conform with the Document type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-zGZVwymUt-J"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n"
          ]
        }
      ],
      "source": [
        "from haystack.utils import clean_wiki_text\n",
        "import pandas as pd\n",
        "\n",
        "harry = pd.read_csv(\"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/harry_potter_wiki.csv\")\n",
        "\n",
        "dicts = []\n",
        "\n",
        "for ix, row in harry.iterrows():\n",
        "    dic = {\n",
        "\n",
        "        'content': clean_wiki_text(row.text),\n",
        "        'meta': {\n",
        "            'name': row['name'],\n",
        "            'url': row.url\n",
        "        }\n",
        "    }\n",
        "    dicts.append(dic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GAYWR9hVk1O"
      },
      "source": [
        "## 2. Launch a Weaviate cluster\n",
        "\n",
        "There are two options to get a Weaviate instance running:\n",
        "1. Run a Weaviate cluster on Weaviate Cluster Service (WCS), which is hosted by SeMI Technologies (for free). You can create an account on https://console.semi.technology. If you have created an account, you can create free a Weaviate instance on https://console.semi.technology/ or you can follow the steps in the code blocks below to create a Weaviate instance. \n",
        "2. Run a Weaviate instance on your local machine. In this case, you need to run this notebook on your local machine too (not as Google Colab, which runs on Google servers). In this case, you can use the Haystack's `launch_weaviate()` function to get Weaviate running on `localhost:8080`, and initiate `WeaviateDocumentStore()` with this host (see below)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzDpf2xmHAG_"
      },
      "source": [
        "### Option 1: Create a Weaviate instance on WCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HQvOnkrFKtL",
        "outputId": "349bfcd1-2ffb-4c1c-fb94-98b5fa22fba0"
      },
      "outputs": [],
      "source": [
        "# Only run if you choose for option 1: Create a Weaviate instance on WCS\n",
        "\n",
        "!pip install weaviate-client==3.3.3 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6asSXN4vHV6G"
      },
      "outputs": [],
      "source": [
        "# Only run if you choose for option 1: Create a Weaviate instance on WCS\n",
        "\n",
        "from getpass import getpass # hide password\n",
        "import weaviate # to communicate to the Weaviate instance\n",
        "\n",
        "from weaviate.wcs import WCS "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jiYk0e8HYPm",
        "outputId": "9174b843-7a3b-4615-e5b9-49fdf51e8295"
      },
      "outputs": [],
      "source": [
        "# Only run if you choose for option 1: Create a Weaviate instance on WCS\n",
        "\n",
        "my_credentials = weaviate.auth.AuthClientPassword(username=input(\"User name: \"), password=getpass('Password: '))\n",
        "my_wcs = WCS(my_credentials)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z22VyH6UIdGr"
      },
      "source": [
        "Now that we connected to WCS, we can `create` a Weaviate instance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "rtMgZi3GHYV5",
        "outputId": "22ec0aef-606d-42a8-aeb7-c9ce2d1df953"
      },
      "outputs": [],
      "source": [
        "# Only run if you choose for option 1: Create a Weaviate instance on WCS\n",
        "\n",
        "with_auth = False # set to true if you don't want your instance to be publically available\n",
        "weaviate_url = my_wcs.create(with_auth=with_auth, wait_for_completion=True) # create a WCS cluster with no vectorization module (ML model) attached (this will be handled by Haystack's EmbeddingRetriever, see below)\n",
        "weaviate_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AISvvrIhV4Sq"
      },
      "source": [
        "### Option 2: Run Weaviate locally\n",
        "\n",
        "Run the cell below onlly if you're usins this notebook locally. This will locally start a Weaviate docker container for you.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrYI3vHYFh4e",
        "outputId": "7ea69cf3-8861-4e31-e093-3ee29ac62358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bdc5d137926c1880ed4271a8ae10f7ba68f9907d1f509377e4e10cdbc6439ada\n"
          ]
        }
      ],
      "source": [
        "# Only run if you choose for option 2: Run Weaviate (and this notebook) locally. Uncomment lines below\n",
        "\n",
        "# from haystack.utils import launch_weaviate\n",
        "# launch_weaviate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FnRqHv_Voml"
      },
      "source": [
        "## 3. Write the documents to a WeaviateDocumentStore\n",
        "\n",
        "Now that you have Weaviate ready,  you can intialize a WeaviateDocumentStore for your Haystack Question Answering pipeline. Depending on which option you picked to start a Weaviate instance above, uncomment and use the relevant bits of code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "IKdoxDjeVrX9",
        "outputId": "6898a757-2ce3-4fdd-b76b-bc2b225b654d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING - haystack.document_stores.weaviate -  Document id 48e81d9a67fd4e0221485586711cc5f0 is not in uuid format. Such ids will be replaced by uuids, in this case c03efb9b-d5fc-9825-35ad-918a543aa525.\n",
            "WARNING - haystack.document_stores.weaviate -  No embedding found in Document object being written into Weaviate. A dummy embedding is being supplied so that indexing can still take place. This embedding should be overwritten in order to perform vector similarity searches.\n",
            "13700it [02:05, 109.30it/s]                           \n"
          ]
        }
      ],
      "source": [
        "from haystack.document_stores import WeaviateDocumentStore\n",
        "\n",
        "# if you choose for option 1 (run Weaviate with WCS)\n",
        "document_store = WeaviateDocumentStore(host=weaviate_url, port=443)\n",
        "\n",
        "# if you choose for option 2 (run Weaviate locally), you can uncomment the line below, and uncomment the line above\n",
        "# document_store = WeaviateDocumentStore() # assumes Weaviate is running on http://localhost:8080\n",
        "\n",
        "document_store.write_documents(documents=dicts, batch_size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIlCbwcHVuWT"
      },
      "source": [
        "## 4. The Retriever\n",
        "\n",
        "Next, we define our [Retriever](https://haystack.deepset.ai/components/retriever). This component acts as a filter to retrieve only the relevant document from your DocumentStore, based on your questy. You have a few options, depending on your usecase and desired accuracy outcomes, you can pick and choose which to use:\n",
        "\n",
        "`EmbeddingRetriever`\n",
        "\n",
        "`DensePassageRetriever`\n",
        "\n",
        "In this example, we're using the `EmbeddingRetriever`, a dense retriever which is able to work with dense vectors (embeddings) and yields great results. This  means we also have to call `update_embeddings()`, which will create the embeddings for our documents in WeaviateDocumentStore. It also means it will take more time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zl3eIxG-VyhX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO - haystack.modeling.utils -  Using devices: CPU\n",
            "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
            "INFO - haystack.nodes.retriever.dense -  Init retriever using embeddings of model sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
            "INFO - haystack.document_stores.weaviate -  Updating embeddings for all 13670 docs ...\n",
            "Batches: 100%|██████████| 4/4 [01:18<00:00, 19.68s/it]\n"
          ]
        }
      ],
      "source": [
        "from haystack.nodes import EmbeddingRetriever\n",
        "\n",
        "retriever = EmbeddingRetriever(document_store=document_store, model_format=\"sentence_transformers\", embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\",)\n",
        "document_store.update_embeddings(retriever)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nu6HI7xV1qy"
      },
      "source": [
        "## 5. The Reader\n",
        "\n",
        "[The Reader](https://haystack.deepset.ai/components/reader) is the component that will be doing the actual Question Answering task. It allows you to use the latest transfromer models which you can find on HuggingFace Model Hub and provide as the `model_name_or_path`. Here, we use the [deepset/tinyroberta-squad2](https://huggingface.co/deepset/roberta-base-squad2) model. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "o08FHhu6V4cK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO - haystack.modeling.utils -  Using devices: CPU\n",
            "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
            "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
            "INFO - haystack.modeling.model.language_model -  =============\n",
            "INFO - haystack.modeling.model.language_model -  Could not find deepset/tinyroberta-squad2 locally.\n",
            "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
            "INFO - haystack.modeling.model.language_model -  Loaded deepset/tinyroberta-squad2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO - haystack.modeling.logger -  ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
            "INFO - haystack.modeling.utils -  Using devices: CPU\n",
            "INFO - haystack.modeling.utils -  Number of GPUs: 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO - haystack.modeling.infer -  Got ya 11 parallel workers to do inference ...\n",
            "INFO - haystack.modeling.infer -   0     0     0     0     0     0     0     0     0     0     0  \n",
            "INFO - haystack.modeling.infer -  /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /|\\  /w\\   /w\\   /w\\ \n",
            "INFO - haystack.modeling.infer -  /'\\   / \\   /'\\   /'\\   / \\   / \\   /'\\   /'\\   /'\\   /'\\   /'\\ \n"
          ]
        }
      ],
      "source": [
        "from haystack.nodes import FARMReader\n",
        "reader = FARMReader(model_name_or_path=\"deepset/tinyroberta-squad2\", use_gpu=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TmMWp4tV9qf"
      },
      "source": [
        "## 6. Initialize an ExtractiveQAPipeline\n",
        "\n",
        "Now we have our Retriever and Reader ready, the only thing left to do is initialize a Question Answering pipeline that uses them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ssKUV4iMV-HL"
      },
      "outputs": [],
      "source": [
        "from haystack.pipelines import ExtractiveQAPipeline\n",
        "\n",
        "pipe = ExtractiveQAPipeline(reader=reader, retriever=retriever)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yI-oEbVWA31"
      },
      "source": [
        "## 7. Ask a question!\n",
        "\n",
        "Now try asking a question to your pipeline 🥳\n",
        "Change the `query` parameter below to ask some thing new, like \"What does McGonagall teach?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-bZJR7pGWC19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.73it/s]\n",
            "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
            "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
            "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
            "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
            "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
            "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
            "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
            "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
            "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
            "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]/Users/laura/.pyenv/versions/3.8.5/lib/python3.8/site-packages/haystack/modeling/model/prediction_head.py:485: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  start_indices = flat_sorted_indices // max_seq_len\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.27s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.33s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.98s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.62s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.06 Batches/s]\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: What does McGonagall teach?\n",
            "Answers:\n",
            "[   <Answer {'answer': 'Transfiguration', 'type': 'extractive', 'score': 0.9635432064533234, 'context': 'James Potter and R. J. H. King.\\nMcGonagall may have been related to Transfiguration teacher Minerva McGonagall. It is possible that M. G. McGonagall i', 'offsets_in_document': [{'start': 327, 'end': 342}], 'offsets_in_context': [{'start': 68, 'end': 83}], 'document_id': '006a7918-9a88-33bf-b685-b7666919e0fc', 'meta': {'name': 'M._G._McGonagall', 'url': 'https://harrypotter.fandom.com/wiki/M._G._McGonagall'}}>,\n",
            "    <Answer {'answer': 'Defence Against the Dark Arts', 'type': 'extractive', 'score': 0.0015463014133274555, 'context': 'ld take moving pictures.\\nHe got his photo with the celebrity Defence Against the Dark Arts teacher Gilderoy Lockhart. However, the photographic Harry ', 'offsets_in_document': [{'start': 1756, 'end': 1785}], 'offsets_in_context': [{'start': 61, 'end': 90}], 'document_id': '0144511c-4e9b-5ae2-0a2d-83aa573e1479', 'meta': {'name': 'Colin_Creevey', 'url': 'https://harrypotter.fandom.com/wiki/Colin_Creevey'}}>,\n",
            "    <Answer {'answer': 'defensive magic', 'type': 'extractive', 'score': 0.0013084025122225285, 'context': 'in and his brother Dennis both joined this alliance and were taught defensive magic by Harry; during their meetings he and his brother were regarded a', 'offsets_in_document': [{'start': 8254, 'end': 8269}], 'offsets_in_context': [{'start': 68, 'end': 83}], 'document_id': '0144511c-4e9b-5ae2-0a2d-83aa573e1479', 'meta': {'name': 'Colin_Creevey', 'url': 'https://harrypotter.fandom.com/wiki/Colin_Creevey'}}>,\n",
            "    <Answer {'answer': 'Minerva attended Hogwarts as a student circa 1947 to 1954, and taught there beginning in December 1956', 'type': 'extractive', 'score': 0.0006310504686553031, 'context': ' Minerva McGonagall, as Minerva attended Hogwarts as a student circa 1947 to 1954, and taught there beginning in December 1956. Nor can it be any of h', 'offsets_in_document': [{'start': 631, 'end': 733}], 'offsets_in_context': [{'start': 24, 'end': 126}], 'document_id': '006a7918-9a88-33bf-b685-b7666919e0fc', 'meta': {'name': 'M._G._McGonagall', 'url': 'https://harrypotter.fandom.com/wiki/M._G._McGonagall'}}>,\n",
            "    <Answer {'answer': 'History of Magic classroom', 'type': 'extractive', 'score': 0.0002778882917482406, 'context': 't it is Class 34 (a Transfiguration classroom) or Class 72 (a History of Magic classroom). Alternatively, since it is off of the Charms corridor, it c', 'offsets_in_document': [{'start': 727, 'end': 753}], 'offsets_in_context': [{'start': 62, 'end': 88}], 'document_id': '010b099a-fab8-deb9-edb8-9416b0c98e04', 'meta': {'name': 'Classroom_off_of_the_Charms_corridor', 'url': 'https://harrypotter.fandom.com/wiki/Classroom_off_of_the_Charms_corridor'}}>]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "prediction = pipe.run(query=\"What does McGonagall teach?\", params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}})\n",
        "\n",
        "from haystack.utils import print_answers\n",
        "\n",
        "print_answers(prediction)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Haystack_QA_for_Weaviate.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
